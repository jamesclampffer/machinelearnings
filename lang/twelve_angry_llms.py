#! /bin/python
# James Clampffer - 2025

"""
TODO: Move to README.md, format as markdown
Like the movie "Twelve Angry Men", except with LLMs.

High level Goals:
1) Get a better feel for LLMs, tooling, HW requirmenets (i.e. runtime
   cost for inference. Identify gaps in my understanding.
2) Quickly stand up a functional, albiet silly, program using based on
   production quality LLMs.
2) Find alternatives to AWS/GCP where getting quota for accelerated
   instances isn't impossible for an individual unlikely to spend more
   than $500/m short term. GH200's @ 1.49/h promo on lambda, and they
   tend to be available in the evening..
3) Watch how quickly an LLM reading LLM generated context begins to
   hallucinate interesting backstory details.
  a) See if there's compounding effect when nearly all the prior text
     in the ctx windows was generated by the same LLM
  b) There's 70 years of adaptations, can a sufficiently large LLM
     infer from this context what the story plot is? Will it try to
     emulate the plot?
  c) Play around with quantization and results on fidelity. IME output
     from 3bit quantized models is superficially similar to having a
     discussion with someone that's had a recent TBI.
5) Concrete example of some open frontier models doing weird stuff in a
   very obvious manner. Are "advanced reasoning models" a VC friendly
   rebrand of RAG?

Basic Design:
1) Each of the 12 "AngryPerson" instances get seeded with a summary of
   their personality. These are sourced from wikipedia.
2) All agents get the same prompt about being on a jury.
3) Jurors are then picked round robin to participate in the discussion.
   a) The shared discussion history just keeps things simple, a logical
      extention would be to allow each to have a history such that
      things can be forgotten on a per-person basis.
   b) Needs improvement, if a juror is asked a question they should be
      scheduled to interact next.
4) Context amalgamation process, the core trick. This builds a single
   context window from three-ish types of info:
   a) Personality as the first (i.e. oldest) bit of context. Immutable.
   b) Prompt to act like a juror, second bit of context. Immutable.
   c) The shared conversation history. Potential for error/"new info"
      injection, dedupe, summarization and age-out to keep the ctx
      window bounded.
   b) A portion of the plot is included to establish common facts.

Future things to try:
1) User specific context history.
   a) Allow bits of context to be forgotten on a per-agent basis.
   b) Use personality to drive what history gets attention vs. what is
      omitted as it ages out.
   c) Add intentionally misremembered history.
2) Trickle in details of the plot over time
"""

import argparse
import ollama
import random
import re
import multiprocessing
import time

# Best effort to determine what sort of HW is available, and pick
# models that'll run on it if no args are provided.
nproc = multiprocessing.cpu_count()
vram_GiB = None
sysram_GiB = None

HAVE_CUDA = False
try:
    import nvidia_smi

    HAVE_CUDA = True
except ImportError:
    print("Cannot find nvidia_smi, run 'pip install nvidia_smi' if using nvidia hw")
    pass

# Take a peak at available VRAM. Needs nvidia_smi lib
if HAVE_CUDA:
    init_iface = False
    try:
        # If multicard, assume all are the same. Also assume no HW
        # contention from other procs.
        iface = nvidia_smi.nvmlDeviceGetHandleByIndex(0)
        meminfo = nvidia_smi.nvmlDeviceGetMemoryInfo(iface)
        # Note: cpu/gpu products with coherent memory e.g. GH200 are
        # only going to look at dedicated vram. With that sort of HW
        # that's still a ton of vram. Keeps things simple + despite
        # cpu/gpu coherence they are banked differently, and vram is
        # going to be markedly faster.
        vram_GiB = meminfo.total / 1024
    except Exception:
        if init_iface:
            nvidia_smi.nvmlShutdown()

# Fall back to system cpu+system memory. Needs psutil lib
# Assumes system ram >> vram size. Don't trust this on containerized
# linux environments - /proc/meminfo does not care about container
# limits.
if not vram_GiB or HAVE_CUDA is False:
    try:
        import psutil

        # Unlike vmem, only look at free - swapping is a bad time
        sysmem = psutil.virtual_memory().free
    except ImportError:
        pass

# all 4 bit K_M quant defaults
QUICK_MODEL = "gemma3:1b"
SMART_MODEL = "gemma3:4b"
if vram_GiB:
    if vram_GiB >= 48:
        QUICK_MODEL = "gemma3:1b"
        SMART_MODEL = "gemma3:27b"
    if vram_GiB >= 32:
        QUICK_MODEL = "gemma3:1b"
        SMART_MODEL = "gemma3:12b"
elif sysram_GiB:
    if sysram_GiB >= 48:
        QUICK_MODEL = "gemma3:1b"
        SMART_MODEL = "gemma3:27b"
    if sysram_GiB >= 32:
        QUICK_MODEL = "gemma3:1b"
        SMART_MODEL = "gemma3:12b"


def extract_question_target(str_resp):
    """Determine if the string contains a question, if it does return who needs to respond"""

    subject_person_prompt = """
    Your job is to identify the name of who is being asked a question.

    You will be given a short block of text from the user in the form of a string
    Expected input
    - It may contain a question or a statement.
    - 1-3 sentences. It will be a short string.

    Rules:
    1) Only return a non-empty string if:
      a) the input contains a question.
      b) the subject of the question has an identifiable name.
      c) the name subject's name is of the form "Juror [0-9] e.g. "Juror 1" or "Juror 12"

    2) If any of the following conditions are true return an empty string
      a) the user string is empty
      b) a name cannot be identified
      c) The name is ambiguous because the string contains many names

    Example strings and associated subjects
    "Juror 1, did you go to the farmer's market?" -> Juror 1
    "I know I didn't say that - did Juror 5 mention it?" -> Juror 5
    "Juror 7, Juror 6 want's to know more about that" -> Juror 5
    "What does Juror 3 have to say?" -> Juror 3

    Strings that aren't questions or don't have named subjects.
    - "Juror 8 went to the farmer's market" -> "", not a question
    - "Did you see it?" -> "", no named subject
    - "I remember Juror 8 said that" -> "", not a question

    Do not respond with a question. Do not respond with elaboration. Only respond with the name.
    """

    res = ollama.chat(
        model=QUICK_MODEL,
        messages=[
            {"role": "system", "content": subject_person_prompt},
            {"role": "user", "content": str_resp},
        ],
    )
    val = res.message.content
    val = val.lower()

    # todo: make an alternation to just match juror 1..12
    match = re.search("juror [0-9]+", val)
    if match:
        s = match.string
        print("Found '{}' in :{}\n".format(s, val))
        return s
    else:
        return None


# From the cast section on wikipedia
jurors = [
    "The foreman; a calm and methodical assistant high school football coach.",
    "A meek and unpretentious bank teller who is easily flustered, but eventually stands up for himself",
    "A hot-tempered owner of a messenger service who is estranged from his son;\
  the most passionate advocate of a 'guilty' verdict.",
    "An unflappable, conscientious, and analytical stockbroker\
 who is concerned only with facts, not opinions.",
    "A Baltimore Orioles fan who grew up in a violent slum,\
  and is sensitive to bigotry towards 'slum kids'.",
    "A tough but principled and courteous house painter who stands up to others,\
 especially over the elderly being verbally abused.",
    "An impatient and wisecracking salesman\
  who is more concerned about the Yankees game he is missing than the case. ",
    "A humane, justice-seeking architect and father of three;\
  initially, the only one to question the evidence and vote 'not guilty'.\
  The closing scene reveals his surname is 'Davis'.",
    "A thoughtful and intelligent elderly man\
 who is highly observant of the witnesses' behaviors and their possible motivations.\
 The closing scene reveals his surname is 'McCardle'. ",
    "A pushy, loud-mouthed and xenophobic garage owner.",
    "A polite European watchmaker and naturalized American citizen who demonstrates strong respect for democratic values such as due process. ",
    "An indecisive and easily distracted advertising executive.",
]

# The initial plot of 12 Angry Men from wikipedia, shortened a little bit
PLOT_SCAFFOLD = [
    """
The trial phase has just concluded for an impoverished 18-year-old boy accused of killing his
abusive father. The judge instructs the jurors they must return a verdict of "not guilty." if
there is reasonable doubt. If found guilty, the defendent will receive a mandatory death sentence.""",
    """A neighbor testified to witnessing the defendant stab his father, from her window, through the windows of a passing elevated train.
Another neighbor testified that he heard the defendant threaten to kill his father, and a body hitting the floor;
he ran to his door and saw defendant running down the stairs. The boy had recently purchased a switchblade that was found,
wiped of fingerprints, at the murder scene, but claimed he lost it.""",
    """In a preliminary vote, all jurors vote "guilty" except Juror 8, who believes there should be some
discussion before the verdict. He says he cannot vote "guilty" because reasonable doubt exists.
When his first few arguments (including producing a recently purchased knife nearly identical to
the murder weapon that was thought to be unique) seemingly fail to convince any of the other jurors,
Juror 8 suggests a secret ballot, from which he will abstain; if all the other jurors still vote
guilty, he will acquiesce. The ballot reveals one "not guilty" vote. Juror 9 reveals that he changed
his vote; he respects Juror 8's motives, and agrees there should be more discussion.""",
    """Juror 8 argues that the train noise would have obscured everything the second witness claimed to have
overheard.""",
]
# NOTE: These give too much info away up front. Could slip them into the context history over time
# """
# Jurors 5 and 11 change their votes. Jurors 5, 6 and 8 further question the second witness's
# story, and question whether the death threat was figurative speech.
# """,
#    """After looking at a diagram of the
# witness's apartment and conducting an experiment, the jurors determine that it is impossible for the
# disabled witness to have made it to the door in time. Juror 3, infuriated, argues with and tries to
# attack Juror 8, yelling a death threat; jurors 5, 6, and 7 physically restrain Juror 3. Jurors 2 and
# 6 change their votes; the jury is now evenly split.
# """,


class DiscussionContext:
    """Rolling history of multiperson dialog, along with anchors"""

    EnableContextSummary = False

    def __init__(self):
        # note: Once this grows large enough old history needs to be aged out into a summary
        self.spoken_history = []

    def add_spoken_str(self, jur_num, stmt):
        s = "Juror {}: {}\n".format(jur_num, stmt)
        if DiscussionContext.EnableContextSummary:
            short = self.summarize(stmt)
        else:
            short = s
        self.spoken_history.append(
            {"role": "user", "content": "Juror {}: {}".format(jur_num, short)}
        )

    def summarize(self, stmt: str) -> str:
        """Get the useful bits of juror statements, they tend to be wordy"""

        SUM_PROMPT = """
        Instructions:
        Below there will be a statement or question. Make this more concise by following the rules listed.
        Only include the condensed statement. In the voice, tone, and affect of the original text. Never prompt for more detail.

        Rules:
        1) Include the names of Jurors in the corresponding output positions if mentioned in input text.
        4) The result should be between 1 and 3 short sentences.
        5) Omit anything about body language e.g. "tapping the desk", that information is not useful here.
        6) If there is repetition in the input that doesn't convey information it can be omitted.
        7) Never respond with "Ok, lets proceed" or similar dialog about completing the task. Only respond with a shortened form of the input.
        """

        # The smaller "QUICK_MODEL" seems to produce more concise output by nature
        res = ollama.chat(
            model=QUICK_MODEL,
            messages=[
                {"role": "user", "content": stmt},
                {"role": "system", "content": SUM_PROMPT},
            ],
        )

        # Sanity check when changing models or making major changes to prompts
        print(
            "\nSummarized:\n\tInput: {}\ninto\n\tOutput: {}".format(stmt, res.message)
        )
        return res.message.content

    def get_dialog_list(self) -> list[dict]:
        """Return the discussion history. May summarize it"""
        buf = [item["content"] for item in self.spoken_history]
        buf.reverse()  # lead with most recent
        return buf


class ContextAmalgamator:
    __slots__ = "discussion_context", "PLOT_SEED"

    def __init__(self, discussion_ctx):
        self.discussion_context = discussion_ctx
        self.PLOT_SEED = PLOT_SCAFFOLD

    def amalgamate(self, jur_num: int, personality: str):
        """Assemble a history for a specific juror
        NOTE:"""

        prolog = [
            'You\'ve been selected as a juror in a murder trial. You will be referred to as "juror {}"'.format(
                jur_num
            ),
            "There are 11 other jurors, Juror 1 is the foreman",
            "Only the jury foreman (Juror 1) may end the discussion",
            "Interpret the facts and discussion through the lens of your personality",
            "Avoid repeating things that have been said recently.",
            "Only Juror 8 votes not guilty initially, he must convince the other 11 jurors",
            "Juror 8 will work to pursuede the other jurors to vote not guilty",
            "Avoid repeating information in a manner that doesn't add to the conversation",
            "Do not include queues for body language or voice",
        ]

        msgs = []

        msgs.append({"role": "system", "content": "You are {}".format(personality)})

        for p in prolog:
            msgs.append({"role": "system", "content": p})

        # These are the oldest, lead with personality, then trial context, and only then
        # the chat history - most recent first.
        for m in self.PLOT_SEED:
            msgs.append(
                {
                    "role": "system",
                    "content": "Here is relevant trial info: {}".format(m),
                }
            )

        convo = []
        for m in self.discussion_context.get_dialog_list():
            v = {"role": "user", "content": m}
            convo.append(v)

        # Oldest stuff towards end. Keep forgetting this isn't a std::vector where a front
        # insert is evil. Still, a deque would make this cleaner without the sort() calls.
        msgs.reverse()

        # Insert a dummy bit of context to delineate where conversation begins
        return (
            convo
            + [{"role": "system", "content": "-" * 50 + "delim" + "-" * 50}]
            + msgs
        )


class AngryPerson:
    __slots__ = (
        "personality",
        "jur_num",
        "spoken_name",
        "conv_history_mngr",
        "ctx_amalgamator",
    )

    def __init__(self, jur_num, conv_history_mngr, amalgamator):
        assert jur_num - 1 in range(12)
        self.personality = jurors[jur_num - 1]
        self.jur_num = jur_num
        self.spoken_name = "Juror {}".format(jur_num)
        self.conv_history_mngr = conv_history_mngr
        self.ctx_amalgamator = amalgamator

    def query_model(self, context_list) -> str:
        """Poke the ollama server"""

        context_list.insert(
            1,
            {
                "role": "system",
                "content": "Only respond from the juror's perspective. One juror per response.",
            },
        )
        resp = ollama.chat(model=SMART_MODEL, messages=context_list)
        return resp.message.content

    def interact(self, q) -> str:
        """Agent interaction based on input + history + context"""
        priors = self.ctx_amalgamator.amalgamate(self.jur_num, self.personality)

        # Don't want to mix in dialog nudge prompts, or at least do not
        # want to persist them.
        for idx, item in enumerate(priors):
            print(("{}: " + item["content"]).format(idx))
        priors.insert(0, {"role": "user", "content": q})

        # Generate dialog, stash it in history
        ans = self.query_model(priors)
        self.conv_history_mngr.add_spoken_str(self.jur_num, ans)
        return ans


def main(interact_count: int, interact_delay: int):
    assert interact_count > 0, "interact_count must be positive"
    jury_discussion = DiscussionContext()
    amalgamator = ContextAmalgamator(jury_discussion)
    jurors = [AngryPerson(i + 1, jury_discussion, amalgamator) for i in range(12)]

    foreman = jurors[0]
    foreman.interact("Let's get started discussing")

    # Place to stash next juror to interact if a question is directed towards them
    next = None
    for epoch in range(interact_count):
        # Pick a juror at random for the next round, unless there's a directed question
        # TODO: Extract question subject, if exists, and schedule that juror for next interaction

        # NB: C++ scoping/declaration habits die hard
        nudge_prompt = None
        if not next:
            # Prompts to keep the interactions on the rails
            jur = jurors[random.randint(0, 11)]
            nudge_prompt = [
                "Make a comment to contribute to deliberations.",
                "Mention an anecdote related to some aspect of the case.",
                "Ask another juror to clarify something they said recently",
                "What would you say to that?",
            ][random.randint(0, 3)]
        else:
            jur = next
            next = None
            nudge_prompt = "Address the last question asked of you."

        # Print next dialog line
        resp = jur.interact(nudge_prompt)
        print("-" * 60 + "\n\t" + resp + "\n\n")
        if interact_delay > 0:
            time.sleep(interact_delay)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="Twelve Angry LLMs",
        description='A set of LLM agents improvise "12 Angry Men"',
    )
    parser.add_argument(
        "--interact_delay",
        type=int,
        default=0,
        help="Insert a delay after each iteration. Useful to read output as it's generated",
    )
    # Use the model that looks like it should fit the hardware, determined above, as the default
    parser.add_argument(
        "--quick_model",
        type=str,
        default=QUICK_MODEL,
        help="Override for model used for background tasks e.g. subject extraction",
    )
    parser.add_argument(
        "--smart_model",
        type=str,
        default=SMART_MODEL,
        help="Override the model used for dialog generation",
    )
    parser.add_argument(
        "--interaction_count",
        type=int,
        default=10,
        help="Number of rounds of generated dialog",
    )
    parsed = parser.parse_args()

    QUICK_MODEL = parsed["--quick_model"]
    SMART_MODEL = parsed["--smart_model"]
    main(parsed["--interaction_count"], parsed["--interact_delay"])
